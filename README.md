# âœ… Auto LLM Response Validator

Automatically validate LLM-generated answers against a local benchmark. Evaluation via semantic similarity and ROUGE.  
Ready to use with **one command**, no external APIs.

---

## âš¡ Quick Start

### ğŸªŸ Windows (PowerShell)

```powershell
./setup_env.ps1
```

### ğŸ§ Linux/macOS (bash)

```bash
chmod +x setup_env.sh
./setup_env.sh
```

These setup scripts:

- ğŸ”§ Create a Python virtual environment
- ğŸ“¦ Install all dependencies
- ğŸ“ Create folder structure and necessary files
- ğŸ“ Generate a default config (`params.json`)

âœ… **Everything that can be automated is automated.**  
ğŸ§  The only thing you need to do is populate `benchmark.json` and `llm_response.json` correctly.

---

## ğŸ“ Project Structure

```bash
data/
â”œâ”€â”€ benchmarks/benchmark.json           # Expected answers
â”œâ”€â”€ llm_response/llm_response.json      # LLM-generated answers
â””â”€â”€ results/results.json                # Final evaluation report

params.json                             # Configuration and thresholds
models/all-MiniLM-L6-v2/                # Local model (included)
```

---

## ğŸ“˜ benchmark.json

```json
[
  {
    "id": "1",
    "question": "Your question here!",
    "answer": "Expected answer here!"
  }
]
```

---

## ğŸ“™ llm_response.json

Supports **one or multiple answers per question**:

```json
[
  { "id": "1", "question": "Your question here!", "answer": "LLM Model Answer 1" },
  { "id": "1", "question": "Your question here!", "answer": "LLM Model Answer 2" }
]
```

---

## ğŸ“Š Output example

```json
{
  "id": "1",
  "question": "...",
  "expected": "...",
  "generated": "...",
  "rouge_score": 0.75,
  "semantic_score": 0.83,
  "passed": false
}
```

Console summary:

```
[âœ”] 22/30 answers passed (threshold = 0.85)
[â„¹ï¸ ] Accuracy: 73.33%
```

---

## ğŸš€ Run the Validation

```bash
python validate_response.py [--json-path] [your-path-params.json]
```